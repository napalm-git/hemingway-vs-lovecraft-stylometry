text <- readLines("texts/hemingway/hem_hills.txt", warn = FALSE)
head(text, 10)
clear
clean
library(stringr)
hemingway_text <- str_c(text, collapse = " ")
library(stringr)
str_sub(hemingway_text, 1, 300)
str_sub(hemingway_text, 1, 300)
lovecraft <- readLines("texts/lovecraft/hpl_cthulhu.txt", warn = FALSE)
lovecraft_text <- str_c(lovecraft, collapse = " ")
str_sub(lovecraft_text, 1, 300)
library(stringr)
hemingway_text <- str_c(text, collapse = " ")
hemingway <- readLines("texts/lovecraft/hem_hills.txt", warn = FALSE)
hemingway <- readLines("texts/hemingway/hem_hills.txt", warn = FALSE)
hemingway_text <- str_c(text, collapse = " ")
lovecraft <- readLines("texts/lovecraft/cthulhu.txt", warn = FALSE)
lovecraft <- readLines("texts/lovecraft/hpl_cthulhu.txt", warn = FALSE)
lovecraft_text <- str_c(lovecraft, collapse = " ")
str_sub(lovecraft_text, 1, 300)
rlang::last_trace()
hemingway_text <- str_c(text, collapse = " ")
hemingway_text <- str_c(hemingway, collapse = " ")
str_sub(hemingway_text, 1, 300)
library(tokenizers)
install.packages(c("mnormt", "psych", "SnowballC", "hunspell", "broom", "tokenizers", "janeaustenr"))
install.packages("tidytext")
library(tokenizers)
avg_sentence_length <- function(txt) {
sentences <- unlist(tokenize_sentences(txt))
words <- sapply(sentences, function(s) length(unlist(tokenize_words(s))))
mean(words)
}
avg_sentence_length(hemingway_text)
avg_sentence_length(lovecraft_text)
View(avg_sentence_length)
install.packages("spacyr")
library(spacyr)
spacy_initialize(model = "en_core_web_sm")
spacy_install()
spacyr::spacy_install(lang_models = "en_core_web_sm")
library(spacyr)
spacy_initialize(model = "en_core_web_sm")
adj_density <- function(txt) {
parsed <- spacy_parse(txt)
adjs <- sum(parsed$pos == "ADJ")
total <- nrow(parsed)
(adjs / total) * 100   # adjectives per 100 words
}
adj_density(hemingway_text)
adj_density(lovecraft_text)
results <- data.frame(
author = c("Hemingway", "Lovecraft"),
avg_sentence_length = c(avg_sentence_length(hemingway_text),
avg_sentence_length(lovecraft_text)),
adj_density = c(adj_density(hemingway_text),
adj_density(lovecraft_text))
)
results <- data.frame(
author = c("Hemingway", "Lovecraft"),
avg_sentence_length = c(avg_sentence_length(hemingway_text),
avg_sentence_length(lovecraft_text)),
adj_density = c(adj_density(hemingway_text),
adj_density(lovecraft_text))
)
library(stringr)
library(tokenizers)
results <- data.frame(
author = c("Hemingway", "Lovecraft"),
avg_sentence_length = c(avg_sentence_length(hemingway_text),
avg_sentence_length(lovecraft_text)),
adj_density = c(adj_density(hemingway_text),
adj_density(lovecraft_text))
)
results
View(results)
adv_density <- function(txt) {
parsed <- spacy_parse(txt)
advs <- sum(parsed$pos == "ADV")
total <- nrow(parsed)
(advs / total) * 100   # adverbs per 100 words
}
adv_density(hemingway_text)
adv_density(lovecraft_text)
pron_density <- function(txt) {
parsed <- spacy_parse(txt)
prons <- sum(parsed$pos == "PRON")
total <- nrow(parsed)
(prons / total) * 100   # pronouns per 100 words
}
pron_density(hemingway_text)
pron_density(lovecraft_text)
subord_density <- function(txt) {
parsed <- spacy_parse(txt, dependency = TRUE)
subs <- sum(parsed$dep_rel == "mark")
total <- nrow(parsed)
(subs / total) * 100   # subordinators per 100 words
}
subord_density(hemingway_text)
subord_density(lovecraft_text)
noun_verb_ratio <- function(txt) {
parsed <- spacy_parse(txt)
nouns <- sum(parsed$pos == "NOUN")
verbs <- sum(parsed$pos == "VERB")
if (verbs == 0) return(NA)   # avoid division by zero
nouns / verbs
}
noun_verb_ratio(hemingway_text)
noun_verb_ratio(lovecraft_text)
# 4. Tense & aspect (percent past tense verbs)
past_tense <- function(txt) {
parsed <- spacy_parse(txt, lemma = FALSE)
pasts <- sum(parsed$tag %in% c("VBD", "VBN"))
total_verbs <- sum(parsed$pos == "VERB")
(pasts / total_verbs) * 100
}
# 7. Typeâ€“token ratio (lexical diversity)
type_token_ratio <- function(txt) {
toks <- tokens(txt, remove_punct = TRUE)
words <- unlist(toks)
length(unique(words)) / length(words)
}
# 8. Proper nouns
proper_noun_density <- function(txt) {
parsed <- spacy_parse(txt)
propers <- sum(parsed$pos == "PROPN")
total <- nrow(parsed)
(propers / total) * 100
}
# 9. Conjunctions
conj_density <- function(txt) {
parsed <- spacy_parse(txt)
conjs <- sum(parsed$pos == "CCONJ" | parsed$pos == "SCONJ")
total <- nrow(parsed)
(conjs / total) * 100
}
# --- Run them ---
past_tense(hemingway_text)
past_tense(lovecraft_text)
type_token_ratio(hemingway_text)
library(quanteda)
install.packages(quanteda)
install.packages("quanteda")
type_token_ratio(hemingway_text)
library(quanteda)
type_token_ratio(hemingway_text)
type_token_ratio(lovecraft_text)
proper_noun_density(hemingway_text)
proper_noun_density(lovecraft_text)
conj_density(hemingway_text)
conj_density(lovecraft_text)
results <- data.frame(
author = c("Hemingway", "Lovecraft"),
avg_sentence_length = c(avg_sentence_length(hemingway_text),
avg_sentence_length(lovecraft_text)),
adj_density = c(adj_density(hemingway_text),
adj_density(lovecraft_text)),
adv_density = c(adv_density(hemingway_text),
adv_density(lovecraft_text)),
pron_density = c(pron_density(hemingway_text),
pron_density(lovecraft_text)),
subord_density = c(subord_density(hemingway_text),
subord_density(lovecraft_text)),
noun_verb_ratio = c(noun_verb_ratio(hemingway_text),
noun_verb_ratio(lovecraft_text)),
past_tense_pct = c(past_tense(hemingway_text),
past_tense(lovecraft_text)),
type_token_ratio = c(type_token_ratio(hemingway_text),
type_token_ratio(lovecraft_text)),
proper_noun_density = c(proper_noun_density(hemingway_text),
proper_noun_density(lovecraft_text)),
conj_density = c(conj_density(hemingway_text),
conj_density(lovecraft_text))
)
results
View(results)
head(spacy_parse(hemingway_text), 20)
rm(list = ls())
gc()
# libs
library(spacyr); library(stringr); library(tokenizers); library(quanteda)
spacy_initialize(model = "en_core_web_sm")
# helpers
read_txt <- function(path) str_c(readLines(path, warn = FALSE), collapse = " ")
features <- function(txt){
parsed <- spacy_parse(txt, dependency = TRUE, lemma = FALSE)
total  <- nrow(parsed)
# sentence length
s <- unlist(tokenize_sentences(txt))
w <- sapply(s, function(x) length(unlist(tokenize_words(x))))
avg_len <- mean(w)
# lexical diversity
toks  <- tokens(txt, remove_punct = TRUE)
words <- unlist(toks)
verbs <- sum(parsed$pos == "VERB")
nouns <- sum(parsed$pos == "NOUN")
data.frame(
avg_sentence_length   = avg_len,
adj_density           = 100 * sum(parsed$pos == "ADJ")   / total,
adv_density           = 100 * sum(parsed$pos == "ADV")   / total,
pron_density          = 100 * sum(parsed$pos == "PRON")  / total,
subord_density        = 100 * sum(parsed$dep_rel == "mark") / total,
noun_verb_ratio       = ifelse(verbs == 0, NA, nouns / verbs),
past_tense_pct        = ifelse(verbs == 0, NA, 100 * sum(parsed$tag %in% c("VBD","VBN")) / verbs),
type_token_ratio      = length(unique(words)) / length(words),
proper_noun_density   = 100 * sum(parsed$pos == "PROPN") / total,
conj_density          = 100 * sum(parsed$pos %in% c("CCONJ","SCONJ")) / total
)
}
# load texts
hemingway_text <- read_txt("texts/hemingway/hem_hills.txt")
lovecraft_text <- read_txt("texts/lovecraft/hpl_cthulhu.txt")
# results
results <- rbind(
cbind(author = "Hemingway", features(hemingway_text)),
cbind(author = "Lovecraft", features(lovecraft_text))
)
results
View(results)
source("C:/Users/ersin/OneDrive/Desktop/MDA_Project/analysis.R", echo = TRUE)
rm(list = ls())
gc()
# libs
library(spacyr); library(stringr); library(tokenizers); library(quanteda)
spacy_initialize(model = "en_core_web_sm")
# helpers
read_txt <- function(path) str_c(readLines(path, warn = FALSE), collapse = " ")
features <- function(txt){
parsed <- spacy_parse(txt, dependency = TRUE, lemma = FALSE)
total  <- nrow(parsed)
# sentence length
s <- unlist(tokenize_sentences(txt))
w <- sapply(s, function(x) length(unlist(tokenize_words(x))))
avg_len <- mean(w)
# lexical diversity
toks  <- tokens(txt, remove_punct = TRUE)
words <- unlist(toks)
verbs <- sum(parsed$pos == "VERB")
nouns <- sum(parsed$pos == "NOUN")
data.frame(
avg_sentence_length   = avg_len,
adj_density           = 100 * sum(parsed$pos == "ADJ")   / total,
adv_density           = 100 * sum(parsed$pos == "ADV")   / total,
pron_density          = 100 * sum(parsed$pos == "PRON")  / total,
subord_density        = 100 * sum(parsed$dep_rel == "mark") / total,
noun_verb_ratio       = ifelse(verbs == 0, NA, nouns / verbs),
past_tense_pct        = ifelse(verbs == 0, NA, 100 * sum(parsed$tag %in% c("VBD","VBN")) / verbs),
type_token_ratio      = length(unique(words)) / length(words),
proper_noun_density   = 100 * sum(parsed$pos == "PROPN") / total,
conj_density          = 100 * sum(parsed$pos %in% c("CCONJ","SCONJ")) / total
)
}
# load texts
hem_hills <- read_txt("texts/hemingway/hem_hills.txt")
hem_cat   <- read_txt("texts/hemingway/hem_cat.txt")
hem_indian <- read_txt("texts/hemingway/hem_indian.txt")
hem_soldier <- read_txt("texts/hemingway/hem_soldier.txt")
hem_bthr <- read_txt("texts/hemingway/hem_bthr.txt")
hpl_colour <- read_txt("texts/lovecraft/hpl_colour.txt")
hpl_dunwich <- read_txt("texts/lovecraft/hpl_dunwich.txt")
hpl_cthulhu <- read_txt("texts/lovecraft/hpl_cthulhu.txt")
hpl_mountain <- read_txt("texts/lovecraft/hpl_mountain.txt")
hpl_outsider <- read_txt("texts/lovecraft/hpl_outsider.txt")
# results
results <- rbind(
cbind(author = "Hemingway", text = "Hills",   features(hem_hills)),
cbind(author = "Hemingway", text = "Cat",     features(hem_cat)),
cbind(author = "Hemingway", text = "Indian",  features(hem_indian)),
cbind(author = "Hemingway", text = "Soldier",     features(hem_soldier)),
cbind(author = "Hemingway", text = "Bthr",  features(hem_bthr)),
cbind(author = "Lovecraft", text = "Colour",  features(hpl_colour)),
cbind(author = "Lovecraft", text = "Dunwich", features(hpl_dunwich)),
cbind(author = "Lovecraft", text = "Cthulhu", features(hpl_cthulhu)),
cbind(author = "Lovecraft", text = "Mountain",  features(hpl_mountain)),
cbind(author = "Lovecraft", text = "Outsider", features(hpl_outsider))
)
results
View(results)
View(results)
library(tidyverse)
library(stringr)
# point to your MAT folder
mat_folder <- "MAT"   # or "Desktop/MAT" depending on where you run R
process_mat_file <- function(path) {
lines <- readLines(path, warn = FALSE)
tokens <- unlist(strsplit(lines, "\\s+"))
# grab MAT feature tags (stuff in [ ])
tags <- str_extract_all(tokens, "\\[.*?\\]") %>% unlist()
tags <- gsub("\\[|\\]", "", tags)
# count them
tag_counts <- table(tags)
total_tokens <- length(tokens)
norm_counts <- (tag_counts / total_tokens) * 100
tibble(file_names = basename(path)) %>%
bind_cols(as.data.frame(t(norm_counts)))
}
files <- list.files(mat_folder, full.names = TRUE, pattern = "\\.txt$|_MAT$")
all_counts <- map_dfr(files, process_mat_file)
# replace NA with 0
all_counts[is.na(all_counts)] <- 0
# save
dir.create("data", showWarnings = FALSE)  # make sure data/ exists
write.csv(all_counts, "data/normalized_postag_counts_renamed.csv", row.names = FALSE)
View(results)
cd ..
write.csv(results, "results/author_features.csv", row.names = FALSE)
spacy_initialize(model = "en_core_web_sm")
write.csv(results, "author_features.csv", row.names = FALSE)
